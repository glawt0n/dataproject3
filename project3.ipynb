{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 3"
      ],
      "metadata": {
        "id": "0gdC70xxFyc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fashion-MNIST image classification using sklearn"
      ],
      "metadata": {
        "id": "i9_7SnpMGKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(len(X_train), -1)\n",
        "X_test  = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "AB136H0PGKq1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "5GAsN-dmHjRM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_configuration(config):\n",
        "    mlp = MLPClassifier(**config)\n",
        "    mlp.fit(X_train, y_train)\n",
        "    y_pred = mlp.predict(X_test)\n",
        "    test_item = {\"config\":config, \"accuracy\":accuracy_score(y_test, y_pred)}\n",
        "    return test_item"
      ],
      "metadata": {
        "id": "bNUCnBBA-1WB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "global population\n",
        "\n",
        "config_ranges = {\n",
        "    \"hidden_layer_sizes\": [(100,), (50), (10, 10), (150)],\n",
        "    \"max_iter\": [7],\n",
        "    \"alpha\": [1e-3, 1e-4, 1e-5],\n",
        "    \"solver\": [\"adam\", \"sgd\"],\n",
        "    \"learning_rate_init\": [0.05, 0.1, 0.15],\n",
        "    \"activation\": ['relu', 'logistic', 'tanh'],\n",
        "    \"early_stopping\": [True, False],\n",
        "    \"random_state\": [1, 2]\n",
        "}\n",
        "numerical_keys = {\"alpha\", \"learning_rate_init\"}\n",
        "all_keys = list(config_ranges.keys())\n",
        "mutable_keys = [key for key in all_keys if len(config_ranges[key]) > 1]\n",
        "\n",
        "mutant_rate = 1/20\n",
        "max_population = 16\n",
        "generations = 8\n",
        "births_per_generation = 8\n",
        "tournament_size = 5\n",
        "population = []\n",
        "\n",
        "def sort_population():\n",
        "    global population\n",
        "    population = sorted(population, key=lambda item: -item[\"accuracy\"])\n",
        "\n",
        "def add_to_population(config):\n",
        "    result = run_configuration(config)\n",
        "    if result not in population:\n",
        "       population.insert(1, result)\n",
        "\n",
        "def random_tournament(ignore={}):\n",
        "    volunteers = [body for body in population if body != ignore]\n",
        "    return max(random.sample(volunteers, min(len(volunteers), tournament_size)), key=lambda body: body[\"accuracy\"])\n",
        "\n",
        "def reproduce(left_body, right_body):\n",
        "    new_config = left_body[\"config\"].copy()\n",
        "    for each_key in random.sample(all_keys, random.randint(0, len(all_keys))):\n",
        "        new_config[each_key] = right_body[\"config\"][each_key]\n",
        "    for each_key in mutable_keys:\n",
        "        if random.uniform(0, 1) < mutant_rate:\n",
        "            if each_key in numerical_keys:\n",
        "                new_config[each_key] *= random.uniform(0.7, 1.3)\n",
        "            else:\n",
        "                new_config[each_key] = random.choice([each for each in config_ranges[each_key] if each != new_config[each_key]])\n",
        "\n",
        "    return new_config\n",
        "\n",
        "def cull_population():\n",
        "    sort_population()\n",
        "    global population\n",
        "    population = population[:max_population]\n",
        "\n",
        "def init_population():\n",
        "    for _ in range(0, max_population):\n",
        "        new_config = {}\n",
        "        for each_key in config_ranges:\n",
        "            new_config[each_key] = random.choice(config_ranges[each_key])\n",
        "        add_to_population(new_config)\n",
        "\n",
        "def display_fitness(generation):\n",
        "    scores = list(map(lambda body: body[\"accuracy\"], population))\n",
        "    print(\"generation\", generation, \"best\", max(scores), \"avg\", sum(scores) / len(scores))\n",
        "\n",
        "# Filter out ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "init_population()\n",
        "display_fitness(0)\n",
        "for generation in range(1, generations + 1):\n",
        "    for _ in range(0, births_per_generation):\n",
        "        left_body = random_tournament()\n",
        "        right_body = random_tournament(left_body)\n",
        "        add_to_population(reproduce(left_body, right_body))\n",
        "    cull_population()\n",
        "    display_fitness(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDtJQ-p32M6z",
        "outputId": "2ded9344-ce31-499e-ba18-eaa809bd54c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generation 0 best 0.8816 avg 0.8157214285714286\n",
            "generation 1 best 0.8824 avg 0.8493625\n",
            "generation 2 best 0.8824 avg 0.85666875\n",
            "generation 3 best 0.8824 avg 0.86456875\n",
            "generation 4 best 0.8824 avg 0.8712562500000001\n",
            "generation 5 best 0.8824 avg 0.87335\n",
            "generation 6 best 0.8837 avg 0.87795\n",
            "generation 7 best 0.8856 avg 0.8788875\n",
            "generation 8 best 0.8856 avg 0.8788875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for each_body in population: print(each_body)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2PjkXACpfYk",
        "outputId": "4b5ec42e-bfc9-414e-f596-5e54012388c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.15177829962856074, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8856}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.15, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8837}\n",
            "{'config': {'hidden_layer_sizes': 150, 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.15, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8824}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.00010938909086935239, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8817}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8817}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 1e-05, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8816}\n",
            "{'config': {'hidden_layer_sizes': 150, 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8799}\n",
            "{'config': {'hidden_layer_sizes': 150, 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 1}, 'accuracy': 0.8792}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': False, 'random_state': 1}, 'accuracy': 0.8791}\n",
            "{'config': {'hidden_layer_sizes': 150, 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.15, 'activation': 'logistic', 'early_stopping': False, 'random_state': 1}, 'accuracy': 0.8783}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.00010938909086935239, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': True, 'random_state': 2}, 'accuracy': 0.8757}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': True, 'random_state': 2}, 'accuracy': 0.8757}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 1e-05, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'logistic', 'early_stopping': True, 'random_state': 2}, 'accuracy': 0.8757}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.00011301175173491971, 'solver': 'sgd', 'learning_rate_init': 0.15, 'activation': 'logistic', 'early_stopping': True, 'random_state': 1}, 'accuracy': 0.8743}\n",
            "{'config': {'hidden_layer_sizes': (100,), 'max_iter': 7, 'alpha': 0.0001, 'solver': 'sgd', 'learning_rate_init': 0.1, 'activation': 'tanh', 'early_stopping': False, 'random_state': 1}, 'accuracy': 0.8743}\n",
            "{'config': {'hidden_layer_sizes': 150, 'max_iter': 7, 'alpha': 0.001, 'solver': 'sgd', 'learning_rate_init': 0.05, 'activation': 'logistic', 'early_stopping': False, 'random_state': 2}, 'accuracy': 0.8733}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yAGuuw06UBl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fashion-MNIST image classification  using pytorch"
      ],
      "metadata": {
        "id": "a2qcKggmIH8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# scale to [0,1], add channel dimension -> (N, 1, 28, 28)\n",
        "X_train = (X_train.astype(\"float32\") / 255.0)[:, None, :, :]\n",
        "X_test  = (X_test.astype(\"float32\")  / 255.0)[:,  None, :, :]\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "# train/val split: last 10k of train as validation\n",
        "X_tr, X_val = X_train[:50000], X_train[50000:]\n",
        "y_tr, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "# wrap in PyTorch TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(y_tr))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)\n",
        "\n",
        "# 2. Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "B9IQwhgcIVOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "f361ec0d-798e-4580-bede-a52e1ef7a54d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1079291198.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 2. Define the loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# In colab, you should ``change runtime type'' to GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# your code here"
      ],
      "metadata": {
        "id": "0REsDBunNmEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define a Convolutional Neural Network (CNN) model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(32 * 7 * 7, 10) # 7x7 comes from the image size after pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten the output for the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "8R1DGSCZdOwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ff1abc"
      },
      "source": [
        "# 6. Evaluate the trained model on the test set\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader: # Evaluate on the test set\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e00cd7b"
      },
      "source": [
        "# 4. Implement the evaluation loop\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader: # Evaluate on the validation set\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef62ddfb"
      },
      "source": [
        "# 3. Implement the training loop\n",
        "num_epochs = 10 # You can adjust this number\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c68fca49"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}